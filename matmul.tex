In the last few years, a lot of work has gone into optimizing matrix multiplication,
especially with floating-point data types
(for example, \cite{nvidia-2024-cublas}, \cite{openblas-2024-openblas}). 
Today, floating point types are faster than
integer data types.
For example, on Nvidia devices, Float32 matrix multiplication
is twice as fast as Int32 matrix multiplication, and Float64
multiplication is supported in hardware while Int64 multiplication
is not.
Moreover, the IEEE standards guarantee that integer multiplication (i.e. those that only use the mantissa) 
is guaranteed to be exact even in floating point types. 
Thus, we can freely treat floating points as an integer type of smaller size.
This trick has been utilized many times in the literature (e.g. see \cite{bglm-2024-matmul-modp}).
Here, we implement a simple version of using this trick in Julia,
which suffices for our purposes.

Let \(\ell\) be the largest possible value of an integer for our data type.
Each entry ranges from \(0\) to \(N-1\).
Thus, the maximum number of operations $o$ before our datatype overflows is one less than the value $o'$ such that 
$o' \cdot (N^2 - 2N + 1) = o^{\prime} \cdot (N-1)^{2} $ is larger than the integer limit $\ell$. Thus
\begin{align*}
    o = \left\lfloor \frac{\ell}{(N^2 - 2N + 1)} \right\rfloor - 1
\end{align*}

For Nvidia GPUs, we wish to use the Float32 type, for the primes
\(p \in \{3,5,7,11,13\}\).
Respectively, for each of these values we have
\(o = 4194302, 599185, 246722, 135299, 85597, 59073\).
Furthermore, the sizes of the matrices in question are 
respectively
 \(165, 969, 2925, 12341, 20825\).
Thus, we see that for \(p \leq 13\), we can use a fully floating-point
library like CUBLAS naively.
\footnote{
    Note that we use OpenBLAS and CUBLAS; 
    we are really using Julia wrappers
    provided by libraries such as Oscar, 
    CUDA.jl, or the Julia standard library.
    Note that MAGMA also provides
    a mod $p$ matrix multiplication implementation,
    which (according to its documentation) wraps CUBLAS for 
    for p=11 and
    p=13. 
}
To support larger primes, we implement a 
simple GPU-based fallback implementation of matrix multiplication using CUDA.jl
based on \cite{mao-2024-matmul}
which reduces mod \(p\) every 32 entries.
Our implementation is provided in GPUFiniteFieldMatrices.jl.
While it doesn't come close to CUBLAS, and is in fact slower
than a OpenBLAS when using CPU multithreading, it gives exact
computations for arbitrarily large
matrices (as long as \(32 < o\))
and is good enough that
matrix multiplication won't be a bottleneck in 
Fedder type criterion calculations.

To illustrate the performance difference between CPU and GPU, we timed
matrix-matrix multiplication of various sizes on the CPU and GPU (an RTX 3060) in 
characteristic 11 (Figure 3). 
Note that the our CPU BLAS tests are called via the default matrix multiplication from Julia and FLINT
(via \texttt{nmod\_matrix}), respectively.

\begin{figure}[h]
\begin{center}
\begin{tabular}{c c c c c}
    \toprule
    Size & 
    CPU, 1 BLAS thread & 
    CPU, 3 BLAS threads & 
    CUBLAS & 
    Fallback GPU implementation \\
           & FLINT default & Julia default & & \\
    \midrule
    5,000  & 2.47   & 1.18  & 0.02 & 1.78   \\
    10,000 & 18.72  & 8.03  & 0.13 & 15.00  \\
    15,000 & 62.90  & 25.30 & 0.43 & 54.95  \\
    20,000 & 150.76 & 58.83 & 3.66 & 114.72 \\
    \bottomrule
\end{tabular}
\caption{Dense square matrix-matrix multiplication times (in seconds) over characteristic $$11$$}
\end{center}
\end{figure}
