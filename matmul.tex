% cut section 5.1, replace with maybe one sentence about what we need to do
% section 5.2, probably shortened in light of edits made to the polynomial multiplication section
% section 5.2, paragraph 2, should mention the implied bit (I (JJ) can explain this if it isn’t clear)
% Everywhere that we say “mod” in this section should become “reduce” or “reduce mod p”
% Table: 
% needs a caption, should be “Table 1”
% I think the table is too big. For the paper, can we shave off the UInt8/Int8 rows and see how it looks?
% The “Largest Int” column should go from scientific notation to algebraic expressions (i.e. 2^32 - 1)
% The numerical columns should not use scientific notation, instead we can use \cdot !0^ (read: times ten to the).
% The table should have a heading that says “Maximum number of additions before overflow” or something to this effect.
% There should be a vertical line between “Largest Int” and the numerical columns
% Everywhere there’s a -1 it should be a blank or a dash
% The figure about max number of operations can go away. The table gives enough info
% the current text of section 5.3 should shrink to one paragraph that cites the places we used to learn the algorithm
% then we should put in the stuff about what we did with modding (er, reducing) here
% section 5.4 can be one sentence at the end of the paragraph that section 5.3 will become, it can just say that we implement a heuristic algorithm in the package
% section 5.5 can move to “computations”

\subsection{Maximum Operations under mod p}

We start by defining notation. Let $A$ be $m \times n$ and $B$ $n \times k$, and $N$ the modulus. One problem with matrix multiplication is the magnitude of each entry in the result $C$, which is at most $n L^2$, where $L$ is the largest entry in $A$ and $B$. When dealing matrices of size $m,n,k \geq 5000$, and large $L$, $n L^2$ may be larger than the numerical datatype used, and thus larger datatypes are necessary. 

In matrix multiplication mod $N$, the maximum size of any element in $C$ is $n (N-1)^2$, which is often smaller than the integer limit of Float32, the most common datatype used in CUDA programming. We take a moment to remark that by IEEE standards, integer multiplication (that only uses the mantissa) is guaranteed to be exact even in Float32 types. As such, it is often preferrable to use Float32 or Float64 datatypes since these are often more optimized than their Int counterparts due to [CITE]. EXPLAIN THE IMPLIED BIT.

Thus, we wish to find the maximum number of operations $o$ before our datatype overflows, namely one less than the value $o'$ such that $o' \cdot (N^2 - 2N - 1)$ is larger than the integer limit $I$. Thus
\begin{align*}
    o = \left\lfloor \frac{I}{(N^2 - 2N - 1)} \right\rfloor - 1
\end{align*}

We list common values of $o$ for the standard Julia datatypes and a selection of values for $N$:

\begin{center}
\begin{tabular}{llrrrrr}
\hline
\textbf{Datatype} & \multicolumn{1}{l}{\textbf{Largest Int}} & \textbf{3} & \textbf{11} & \textbf{101} & \textbf{8191} & \textbf{9765625} \\ \hline
Float16           & 1.02E+03                                     & 510        & 9           & -1           & -1            & -1               \\
Float32           & 8.39E+06                                     & 4.19E+06   & 8.56E+04    & 838          & -1            & -1               \\
Float64           & 4.50E+15                                     & 2.25E+15   & 4.60E+13    & 4.50E+11     & 6.71E+07      & 46               \\
Int16             & $2^{15}-1$                                     & 16382      & 333         & 2            & -1            & -1               \\
Int32             & $2^{31}-1$                                     & 1.07E+09   & 2.19E+07    & 2.15E+05     & 31            & -1               \\
Int64             & $2^{63}-1$                                     & 4.61E+18   & 9.41E+16    & 9.23E+14     & 1.38E+11      & 96713            \\
UInt16            & $2^16-1$                                     & 32766      & 667         & 5            & -1            & -1               \\
UInt32            & $2^{32}-1$                                     & 2.15E+09   & 4.38E+07    & 4.30E+05     & 63            & -1               \\
UInt64            & $2^{64}-1$                                     & 9.22E+18   & 1.88E+17    & 1.85E+15     & 2.75E+11      & 1.93E+05         \\ \hline
\end{tabular} \bigskip\\
\textbf{Table 1:} Maximum Number of Additions Before Overflow
\end{center}
%

Knowing $o$ helps us choose the smallest datatype such that matrix multiplication without overflow. In larger cases, we may need to reduce the current result before adding more elements. This common technique allows us to use considerably smaller datatypes, though it comes with cost of needing to reduce the result every so often.

\subsection{GPU and CUDA Considerations}

As described in [CITE], there are many additional considerations when doing matrix multiplication on the gpu. First, one should minimize memory access by storing commonly used items in cache or shared memory. Second, computations should be parallelized across groups of 32 threads, the smallest warp or unit call in CUDA. Third, conditionals and other control flow statements should be avoided in GPU kernels, as these are costly on the GPU. However, from our testing, we find it is worth checking whether our output needs to be reduced rather than reducing at every addition. Then, based on the input size, we end up with four regimes:
\begin{enumerate}
    \item Matmul on the CPU, if the time to move to the GPU and back is larger than computing on the CPU.
    \item CUBLAS's Matmul, if the matrix and modulus is small enough that we do not need to reduce partway.
    \item Reducing after every block, which allows us to get rid of the costly if statement.
    \item Reducing based on a conditional every addition, for extreme cases where $o < 32$.
\end{enumerate}
We implement this heuristic algorithm in the package.

% \subsection{Benchmarks and Comparisons}

% TODO: Compare to other implementations e.g. MAGMA

% We offer two ways to compare our implementation with other options. One is through raw speed in the amount of time it takes to complete the operation, and the other is by counring the number of floating point operations (flops) or Finite Field operations (Flops) completed per second. We remark that the number of flops is always larger than Flops, and that this number depends on the regime chosen.