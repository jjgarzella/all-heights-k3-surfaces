
To find the matrix of ``multiply then split''
in the Algorithm for the quasi-\(F\)-split height
of a Calabi-Yau hypersurface, 
we must first multiply all possible 
monomials by \(\Delta_{1}(f)\) 
and then apply the map \(u\).
Here, we consider a slightly more general problem.
As usual, let \(S = k[x_{1}, \ldots, x_{n}]\).
Let $S_\ell$ denote the vector space of homogeneous degree $\ell$ polynomials in \(S\).
We let \(B_{\ell}\) denote a lexographically-ordered list
of the monomial basis of \(S_{\ell}\).
Fix some \(D \in \mathbb{N}\), and let \(\Delta \in S_D\).
Furthermore, fix \(d \in \mathbb{N}\).
We consider the problem of finding the matrix of 
\(g \mapsto u(\Delta g)\)
on the space \(S_{d}\).
The target of this map is
\(S_{d^{\prime}}\), where
\(d^{\prime} \colonequals \frac{d+D-n-1}{p}\).
By convention, if \(d^{\prime}\) is not an integer,
we declare \(S_{d^{\prime}}\) to be zero.
Note that in this case there are
there are no terms which survive \(u\), so
\(g \mapsto u(\Delta g)\) is indeed the zero map.

\begin{rmk}
    If \(\Delta = \Delta_{1}(f^{p-1})\) and \(d = n(p-1)\),
	as in the case of calculating the quasi-\(F\)-split
	height of a Calabi-Yau hypersurface, 
	then \(d^{\prime} = d\) and the matrix is 
	square.
\end{rmk}

Our first observation is that naively 
multiplying and then applying \(u\) 
as in Algorithm \ref{alg:naive:u}
does plenty of unnecessary work.
In particular, any term in the 
product \(\Delta g\) with exponents 
not congruent to 
\((p-1, \ldots, p-1) \mod p\)
is not needed. 
Even if \(g\) is a monomial, giving a linear
algorithm for multiplication, using naive
multiplication stores a large amount of unnecessary terms
in memory.
Both of our improved algorithms ignore
all of these unnecessary terms.

In what follows, 
when we apply arithmetic operations to arrays, 
(particularly addition, multiplication, and modulo)
we mean componentwise application of these operations.
Use of arbitrary componentwise operations is called
\textit{broadcasting} in Julia, and we will
sometimes use this term.
Homogenoeus polynomials are represented in computer 
memory as a list of their terms. 
This is reflected in our notation, 
thus the meaning of ``for \(\delta \in \Delta\)''
is a for loop though all the monomials
of \(\Delta\).

\begin{defn}
    \label{def:poly:nota}
    The \textit{exponent tuple} of a monomial 
    $m = ax_{1}^{d_1}, \dots, x_{n}^{d_n}$ is $(d_1, \dots, d_n)$. 
	We will denote it $\exps(m)$, and we also 
    define $\coeff(m) \colonequals a$
\end{defn}

\begin{defn}
    The weak integer compositions of $k$ into $n$ parts, 
    denoted $\wics(k, n)$, is the set of ordered tuples 
	\((d_{1}, \ldots, d_{n})\) such that 
    \(d_{1} + \ldots + d_{n} = k\).
\end{defn}

\begin{ex}
    $\wics(2, 3) = \lbrace (2, 0, 0), (0, 2, 0), (0, 0, 2), (1, 1, 0), (1, 0, 1), (0, 1, 1) \rbrace$.
\end{ex}

Thus, $\wics(d, n)$ generates the exponent tuples of the 
monomial basis of the vector space of $d$-homogeneous 
$n$-variate polynomials over a field $F$.
It follows that the dimension of the vector space is $|\wics(k, n)|$.

\begin{lem}
    \label{lem:wics:size}
    $|\wics(k, n)| = \binom{k + n - 1}{n - 1}$
\end{lem}

\begin{proof}
	This is a classical argument which goes by the 
    name ``sticks and stones,'' ``stars and bars,'' or 
    ``dots and dividers.''
\end{proof}


\begin{rmk}
    The number of terms of \(\Delta\) is bounded 
    above by \(\binom{D+n-1}{n-1}\) by Lemma \ref{lem:wics:size}.
    Likewise, the number of elements of 
    \(B_{d}\) is \(\binom{d+n-1}{n-1}\).
    For all of our algorithmic complexity calculations, 
    we let $\ell_{d} = \length(B_{d}) = \binom{n + d - 1}{n - 1}$, 
    and $\ell_{\Delta} = \length(\Delta) \leq \binom{n + D - 1}{n - 1}$.
    In the case of the quasi-\(F\)-split height of a K3 surface, 
    $\ell_{d} = \binom{n(p - 1) + n - 1}{n - 1} = \binom{np - 1}{n - 1}$, 
    and $\ell_{\Delta} = \binom{np(p - 1) + n - 1}{n - 1} = \binom{n(p^2 - p + 1) - 1}{n - 1}$.
\end{rmk}

\begin{defn}
	Let \(m\) be a monomial in \(S\). 
	Then we say that \(m\) \textit{matches}
	with another monomial \(m^{\prime}\) 
	if the monomial \(mm^{\prime}\) is
	not killed by \(u\).
\end{defn}

Concretely, this means that $\exps(m) + \exps(m^{\prime}) \equiv (p - 1, \dots, p - 1) \mod p$.

\subsection{The trivial algorithm}

\begin{algorithm}[H]
    \caption{Matrix of multiply then split: \triv}
    \label{alg:matrix:trivial}
    \KwInput{
        $\Delta, B_d, B_{d^\prime}, p$
    }
    \KwNotation{Let $\ell_d = \code{length}(B_d), \ell_{d^\prime} = \code{length}(B_{d^\prime})$}
    \KwOutput{$\ell_{d^\prime} \times \ell_d $ matrix representing ``multiply then split''}
    $M \gets \text{zero matrix of size } \ell_{d^\prime} \times \ell_d $\;
    \For{$m \in B_d$}{
        \For{$\delta \in \Delta$}{
            \Comment{If $\delta$ matches with $m$}
            \If{$\code{exps}(\delta) + \code{exps}(m) \equiv (p - 1, \dots, p - 1) \mod p$}{
                \Comment{Apply $u$ to $\delta m$}
                $\text{res} \gets (\code{exps}(\delta) + \code{exps}(m) - (p - 1, \dots, p - 1)) / p$\;
                $\text{row} \gets \code{indexof}(\text{res},B_{d^{\prime}})$\;
                $\text{col} \gets \code{indexof}(m, B_d)$\;
                $M[\text{row}, \text{col}] = \code{coeff}(\delta)$
            }
        }
    }
    \Return $M$
\end{algorithm}

In this algorithm, which we call \triv, we iterate through 
the monomials $m \in B_{d}$, each corresponding to a column in the
resulting matrix. For each monomial, we search for terms 
that match $\delta \in \Delta$, apply $u$ to their product $\delta m$, and 
get the lexographical index of the result to find which 
row to add to.

In this algorithm, the matrix can be generated in 
$O(\ell_{d}\ell_{\Delta})$ operations. 
This can be
seen from the nested loops, assuming that integer 
arithmetic operations are constant time.

In practice, the majority of the combinations of 
terms of $\Delta$ and $B_{d}$ don't match. This means in
terms of $\Delta$ and $B_{d}$ don't match. This means in
Algorithm \ref{alg:matrix:trivial}, much of our 
runtime is wasted on checking for whether terms match.
However, we emphasize that this algorithm, if 
implemented in parallel on the GPU, is indeed
fast enough to not be a bottleneck in practice.

\subsection{Modified merge-based algorithm}

We now introduce an algorithm that utilizes 
properties of monomial ordering to reduce the 
number of comparisons 
performed in checking whether two terms match.

\begin{lem}
	\label{lem:tuples:modp}
	Let \(X = (x_{1}, \ldots, x_{n}),
	Y = (y_{1}, \ldots, y_{n})\)
	be in \(\{0, \ldots, p-1\}^{n} \ins \mathbb{Z}^{n}\).
	Then \(X + Y = 
	(p-1, \ldots, p-1) \mod p\)
	if and only if 
	\(X + Y = 
	(p-1, \ldots, p-1)\).
\end{lem}

\begin{proof}
	Each coordinate has \(x_{i} + y_{i} \leq 2(p-1) = 2p-2\).
\end{proof}

\begin{cor}
    \label{cor:unique:match}
	Let $X$ be as above.
	Then there exists a unique
	match \(m(X) \in \{0, \ldots, p-1\}^{n}\) 
\end{cor}

\begin{proof}
	\(m(X) = 
	(p-1, \ldots, p-1) - X\).
	The claim follows from 
	\ref{lem:tuples:modp}.
\end{proof}

Essentially, this corollary says that
when we consider the exponent tuple mod $p$ of 
an arbitrary monomial of \(B\),
it has a unique
match mod \(p\).

\begin{cor}
	\label{cor:match:order}
	Let \(\leq_{lex}\) denote the 
	lexographical ordering.
	If \(X \leq_{lex} Y\),
	then 
	\(m(Y) \leq_{lex} m(X)\)
\end{cor}

\begin{proof}
	Follows from the definition of lexographical
	ordering.
\end{proof}

Using these two corollaries, we have the following algorithm,
which we call \merge:

\begin{algorithm}[H]
\caption{Matrix of multiply then split: \merge}
\label{alg:theta:merge}
\KwInput{
    $\Delta, B_d, B_{d^\prime}, p$
}
\KwNotation{Let $\ell_d = \code{length}(B_d), \ell_{d^\prime} = \code{length}(B_{d^\prime})$}
\KwOutput{$\ell_{d^\prime} \times \ell_d $ matrix representing ``multiply then split''}
$M \gets \text{zero matrix of size } \ell_{d^\prime} \times \ell_d $\;
$L \gets [\code{exps}(\delta) \modp ~|~ \delta \in \Delta]$\;
$R \gets [\code{exps}(m) \modp ~|~ m \in B_{d}]$\;
\Comment{Lexicographically sort $L$ and $R$ and keep the permutations}
$P_1 =$ \code{sortperm}\((L)\), $P_2 =$ \code{sortperm}\((R)\) \;
\Comment{Permute \(\Delta\) and \(B_d\)}
$\Delta^{\prime} \gets \Delta[P_1], B_d^{\prime} \gets B_d[P_2]$\;
\(l \gets 1, r \gets \code{length}(R)\)\;
\While{\(r \geq 1 ~\textnormal{and}~ l \leq \code{length}(L)\)}{
    cmp \(\gets L[l] + R[r] - (p-1, \ldots, p-1)\)\;
    \uIf(\tcp*[h]{If lex sum of pair too small, increment index of $L$}){\textnormal{cmp < 0}}{
        \(l \gets l + 1\)\;
    }\uElseIf(~\tcp*[h]{If lex sum of pair too big, decrement index of $R$}){\textnormal{cmp > 0}}{
        \(r \gets r - 1\)\;
    }\Else(~\tcp*[h]{Match found}){
        matchr \(\gets R[r]\)\;
        numLmatches \(\gets\) the number of adjacent entries in \(L\) which are equal to \(L[l]\)\;
        \Comment{For each monomial of $B_d$}
        \While{$R[r] ~\code{==}~ \textnormal{matchr} ~\textnormal{and}~ 1 \leq r$}{
            \Comment{Find matching terms of $\Delta$}
            \For{\(ll \in \{l, l+1, \ldots, l+\textnormal{numLmatches} - 1\})\)}{
                \Comment{And apply $u$}
                res \(\gets (\code{exps}(\Delta^{\prime}[ll]) + \code{exps}(B^{\prime}[r]) - (p-1, \ldots, p-1)) / p\)\;
                \(\text{col} \gets \code{indexof}(B^{\prime}[r],B_{d})\)\;
                \(\text{row} \gets \code{indexof}(\text{res},B_{d^{\prime}})\)\;
                \(M[\text{row},\text{col}] = \code{coeff}(\Delta^{\prime}[ll])\)\;
            }
            \(r \gets r - 1\)\;
        }
        \(l \gets l + \text{numLmatches} - 1\)\;
    }
}
\Return $M$
\end{algorithm}

Note that the inner while loops account for the fact that
after modding by \(p\), one does not expect 
either array to have unique elements.
The algorithm is justified by the previous corollaries.

In practice, instead of sorting the arrays in place, 
we use a method like Julia's \texttt{sortperm}
to get the sort permutation.

Because we perform two sorts, then a linear merge, 
we perform 
$\ell_{d} \log \ell_{d} + \ell_{\Delta} \log \ell_{\Delta} + \ell_{d} + \ell_{\Delta}$ operations, 
giving the algorithm complexity 
$O(\max(\ell_{d} \log \ell_{d}, \ell_{\Delta} \log \ell_{\Delta}))$.
In practice (for Calabi-Yau hypersurfaces), 
the $\ell_{\Delta} \log \ell_{\Delta}$ usually dominates.

\subsection{Weak integer compositions-based algorithm}

\begin{lem}
    \label{lem:generate:matching}
    The set of exponent tuples of \(B_{d}\) that match with $\delta$ is given by 
    \[
        \left\{ m(\exps(\delta) \mathmod p) + p\cdot w ~\Big|~ w \in \wics 
        \left(\frac{d - sum(m(\exps(\delta) \mathmod p))}{p}, n \right) \right\}
    \]
    where $sum(X)$ is the sum of the elements of tuple $X$, and $m$ is the matching function from Corollary \ref{cor:unique:match}
\end{lem}

\begin{proof}
    Direct computation from the definitions of weak integer compositions and matching terms.
\end{proof}


Because this expression is quite convoluted, we 
provide an example of a computation 
that may come up in calculating the quasi-\(F\)-split 
height of a K3 quartic surface over $\mathbb{F}_5$.

\begin{ex}
    Let $p = 5$, \(n = 4\), and $\delta$ a monomial 
    with exponent tuple $(21, 19, 22, 18)$. 
    Then $B_{16}$ is the monomial basis of 
    \(S_{16}\) (over $\mathbb{F}_5$). 
    To find the monomials of $B_{16}$ that match with 
    $\delta$, we begin by reducing $(21, 19, 22, 18) \mathmod 5 = (1, 4, 2, 3)$.
    This makes $m((1, 4, 2, 3)) = (3, 0, 2, 1)$ the 
    exponent tuple of a matching monomial mod \(p\).
    However, \((3, 0, 2, 1)\) isn't in $B_{16}$. 
    For it to be in $B_{16}$, we need to add $16 - (3 + 0 + 2 + 1) = 10$ 
    to the degree, and to maintain congruence to 
    $(4, 4, 4, 4) \mathmod 5$, we need to add two $5$'s 
    to the elements of $(3, 0, 2, 1)$. This corresponds 
    to $\{5w ~|~ w \in \wics(2, 4)\}$, which is:
    \begin{align*}
        \{&(10, 0, 0, 0), (5, 5, 0, 0), (5, 0, 5, 0), (5, 0, 0, 5), (0, 10, 0, 0), \\
        &(0, 5, 5, 0), (0, 5, 0, 5), (0, 0, 10, 0), (0, 0, 5, 5), (0, 0, 0, 10)\}
    \end{align*}
        
    \noindent Adding each of these to $(3, 0, 2, 1)$, we get the 
    exponent tuples of monomials that match with $(21, 19, 22, 18)$:
    \begin{align*}
        \{&(13, 0, 2, 1), (8, 7, 0, 1), (8, 0, 7, 1), (8, 0, 2, 6), (3, 10, 2, 1), \\
        &(3, 5, 7, 1), (3, 5, 2, 6), (3, 0, 12, 1), (3, 0, 7, 6), (3, 0, 2, 11)\}
    \end{align*}    
\end{ex}

Because we need it later for our complexity computations, we have the corollary:

\begin{cor}
    \label{cor:num:matches}
    The number of matching monomials of any term $\delta$ is 
    bounded above by $\binom{\lfloor \frac{d}{p} \rfloor + n - 1}{n - 1}$.
\end{cor}

\begin{proof}
    By Lemma \ref{lem:generate:matching}, there are 
    $\Big |\wics \left(\frac{d - sum(m(\exps(\delta) \mathmod p))}{p}, n \right) \Big |$ 
    monomials that match with $\delta$.
    Because the division is exact, 
    $\lfloor \frac{d}{p} \rfloor \geq \frac{d - sum(m(\exps(\delta) \mathmod p))}{p}$,
    giving the upper bound 
    $|\wics(\lfloor \frac{d}{p} \rfloor, n)| = \binom{\lfloor \frac{d}{p} \rfloor + n - 1}{n - 1}$.
\end{proof}

Denoting the expression from Lemma \ref{lem:generate:matching} 
by \code{generateMatchingMonomials}$(\delta, p, M_d)$, we have
the following algorithm, which we call \wicsalg:

\begin{algorithm}[H]
    \caption{Matrix of multiply then split: \wicsalg}
    \KwInput{$\Delta, B_{d}, B_{d^{\prime}}, p$}
    \KwNotation{Let \(\ell_{d} = \length(B_{d})\), 
    \(\ell_{d^{\prime}} = \length(B_{d^{\prime}})\)}
    \KwOutput{$\ell_{d^{\prime}} \times \ell_{d}$ 
    	matrix representing ``multiply then split''}
    $M \gets \text{zero matrix of size } \ell_{d^\prime} \times \ell_d $\;
    \For{$\delta \in \Delta$}{
        mons $\gets \code{generateMatchingMonomials}(\delta, p, B_{d})$\;
        \For{$m \in \textnormal{mons}$}{
            $\text{col} \gets \code{indexof}(m,B_{d})$\;
			res $\gets (\code{exps}(\delta) + \code{exps}(m) - (p-1, \ldots, p-1)) / p$\;
			$\text{row} \gets \code{indexof}(\text{res},B_{d^{\prime}})$\;
            $M[\text{row}, \text{col}] = \code{coeff}(\delta)$\;
        }
    }
    \Return $M$
    \label{alg:matrix:WICS}
\end{algorithm}

This algorithm 
generates the matrix in 
$O \left( \ell_{\Delta} \cdot \binom{\lfloor \frac{d}{p} \rfloor + n - 1}{n} \right)$ operations.
The number of matching monomials each term $\delta$ 
has is bounded above by $\binom{\lfloor \frac{d}{p} \rfloor + n - 1}{n}$ 
by Corollary \ref{cor:num:matches}, which is 
effectively constant for all reasonably computable $n$ and $p$

\subsection{Implementation}

In the implementation of \triv, \merge, and \wicsalg, there are a 
few optimizations and considerations common to all the implementations.
First, all exponent tuples are bitpacked into 
unsigned integers, improving memory efficiency 
and allowing for faster comparisons.
Note that a comparison between two unsigned integers 
is the same as a lexicographical comparison between the tuples.
Broadcasted addition and subtraction on bitpacked 
$n$-tuples reduces to addition and subtraction of 
integers, though broadcasted division and modulo still 
require $n$ separate operations.

All algorithms require the \code{indexof}$()$ function, 
which we implement using Julia's \texttt{Dict\{K,V\}}, 
which is implemented as a hashtable.

\triv ~and \wicsalg ~are easily GPU-parallelizable by creating 
a thread for each term of $\Delta$, and performing the insides of the loop in each thread.
This creates the problem of needing an \code{indexof}$()$ function, 
since Julia's \texttt{Dict\{K,V\}} is not compatible with the GPU.
The solution we implement is a static hashtable, with size 
and hashing function decided at compiletime.
Due to Julia's JIT compilation nature, creating this 
hashtable is quite slow.
Another solution can be to perform a binary search 
for the \code{indexof}$()$ function, since $B_d$ is lexicographically 
sorted, but for one-off computations, users are better 
off using non-parallelized \merge ~or \wicsalg ~to avoid 
first-time GPU kernel compilation times.
However, if many matrices need to be created, caching 
the hashtable allows the GPU implementations to far outpace 
the non-parallelized CPU implementations.

\merge ~should be parallelizable by implementing a modified parallel merge and using a parallel sort. 
However, we expect the sorting bottleneck to mean that such a parallel algorithm wouldn't beat GPU-parallel \wicsalg,
so we don't bother developing the modified parallel merge.

Below in Figure \ref{fig:momts:compare}, we compare running times 
for all the \(p\) which we consider. 
\wicsalg \, ends up being the fastest practically, 
both on the CPU and the GPU.
We did not try running the tests for \triv~ on the 
CPU for \(p=11,13\) because we do not expect 
it to finish in a reasonable
amount of time.

\begin{figure}[ht]
\label{fig:momts:compare}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    p & \triv ~- CPU & \triv ~- GPU & \merge   & \wicsalg ~- CPU & \wicsalg ~- GPU \\
    \hline
    3 & 0.01   & 0.0007  & 0.001  & 0.001   & 0.0002  \\
    \hline                                          
    5 & 1.6    & 0.005   & 0.046  & 0.028   & 0.0024  \\
    \hline                                          
    7 & 45.33  & 0.104   & 0.670  & 0.277   & 0.025  \\
    \hline                                          
    11 &    -  & 6.796   & 26.35  & 5.45    & 0.56  \\
    \hline                                          
    13 &    -  & 31.86   & 88.97  & 17.06   & 2.05 \\
    \hline
\end{tabular}
\caption{Comparison of timings for various algorithms that calculate
the matrix of ``multiply then split''. 
Times are in seconds
and are an average of 10 different trials.}
\end{center}
\end{figure}

