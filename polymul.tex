In this section, we describe how Algorithm \ref{alg:calc:delta1} 
is computed. Specifically, we focus on Step 3, 
\(D \gets \tilde{f}^{p}\), which is the main 
bottleneck. We first explain our implementation 
of polynomial powering using the Number Theoretic 
Transform (NTT), then explore other known polynomial 
powering algorithms and their ability to be sped up 
through mass parallelization.

\subsection{Multi-modular NTT}

\subsubsection{Kronecker Substitution}
We reduce the problem of multivariate 
polynomial powering to univariate polynomial powering
by the Kronecker substitution.
Let $M$ be a non-inclusive upper bound on the degree of any variable of the polynomial
$f \in R[x_1, \dots, x_n]$. 
Then the Kronecker Substitution 
\(g(z) = f(z, z^M, z^{M^2}, \dots, z^{M^{n-1}})\)
produces a univariate polynomial $g \in R[z]$,
see \cite{arnold-2014-kronecker}.
Algebraically, this is the same as applying the 
homomorphism 
\(R[x_{1}, \ldots, x_{n}] \xrightarrow{} R[z]\)
which takes \(x_{i}\) to
\(z^{M^{i-1}}\).
This map cannot be injective in general, but it
is injective on the subset of elements of
\(f \in R[x_{1}, \ldots, x_{n}]\) 
whose terms have all degrees strictly less than \(M\).
Given two polynomials \(f_{1}, f_{2}\) that
we wish to multiply, we may choose \(M\)
big enough
so that the product lies in the subset,
so we may recover the product in 
\(R[x_{1}, \ldots, x_{n}]\) by performing
the product in the ring \(R[z]\).

\begin{rmk}
    In FLINT, exponent tuples are bitpacked into unsigned 
    integers. The process of bitpacking an array is a 
    Kronecker Substitution with $M$ being a power of 2.
\end{rmk}

In the case where our polynomial is homogeneous, 
we can simply ignore one variable in all of our 
operations between monomials of the same degree. 
As above, algebraically this is applying the evaluation
homomorphism \(x_{n} \mapsto 1\).
This map is of course not injective, but it is injective on homogeneous elements.
This effectively decreases the number of variables of our polynomial by 1, 
which greatly lowers the degree of the univariate result of the
Kronecker Substitution.
This improvement is essential for dense algorithms like the NTT.

\subsubsection{Polynomial Powering using NTTs}
The \(k\)-th power of a univariate polynomial \(f\)
may be computed mod \(p\) by taking the NTT 
of a tuple containing the coefficients of \(f\),
raising the components to the \(k\),
and then computing the Inverse Number Theoretic Transform.
The multimodular algorithm 
computes \(f^{k}\) in the integers by
choosing primes \(p_{i}\), 
and combining the results using the Chinese remainder theorem.
Thus, our algorithm is known as the 
multi-modular NTT approach to raising polynomials to powers.

In many cases, using the Kronecker Substitution to 
map a multivariate polynomial to a univariate polynomial results in 
a sparse univariate polynomial. As such, dense 
algorithms like the NTT become inefficient in both 
time and memory complexity when compared to sparse algorithms.
However, the massive throughput of GPUs allows the 
NTT to be competitive for many cases, 
quasi-F-split heights of quartic K3 surfaces being 
one of them.

\subsubsection{NTT Implementation}

We use the Merge-NTT algorithm with Barrett modular 
reduction from \cite{ozcan-2023-fft} 
ported to Julia for our NTT implementation. 
So, our NTT length $L$ is always a power of $2$. 

For each NTT we perform, we search for primes $p$ that satisfy 
$p \equiv 1 \mod L$, compute primitive $L$th roots of unity 
for each $p$, and $L^{-1} \mod p$. 
These are cached, so that they can be used again for problems with the same shape 
(i.e. the degree of \(f\), the exponent \(k\), and the characteristic are the same).

Our algorithm also has fallbacks when the memory required becomes too large for the 
GPU's on-device memory.
When the problem becomes too big to fit the twiddle factors and 
inputs for all of the NTTs in device memory at once, 
we move the inputs to GPU memory and 
back to RAM for each NTT, and don't cache the twiddle factors. 
Because we are forced to move a lot of memory around for each NTT, 
and run $\log_2(L)$ modular exponentiations in each thread,
we see a large drop-off in 
performance when this NTT size threshold is reached. 


\subsubsection{Prime Selection}
In order for the NTT to simulate polynomial powering over $\mathbb{Z}$, 
we need to obtain an upper bound $M$ 
on the resulting coefficients, then 
choose primes $p_1 \dots p_k$ such 
that $p_1 \cdot p_2 \cdot ... \cdot p_k > M$.

Experimentally, modular multiplication with 32-bit integers 
is slightly over double the speed of modular 
multiplication with 64-bit integers. 
However, the kernels and parameters from \cite{ozcan-2023-fft} are more 
optimized for 64-bit integers, making them just over half the
speed of naively using the same implementation with 32-bit 
integers. Thus, for prime selection, 
we choose primes satisfying $p \equiv 1 \mod L$ that fit 
within 62 bits, because of the precision of Barrett Reduction.

\subsubsection{Bound Finding}
%This poses the question of finding the maximum coefficient of our result $M$. 
The multimodular NTT requires an upper bound $M$ in order to select primes 
to compute NTTs in. In this section, we present a quick way to compute a 
relatively tight upper bound on the resulting 
coefficients of raising a homogeneous polynomial to a power, which
will allow for easier application of the NTT in polynomial powering
problems.

\begin{defn}
    Given a basis $(\mathbf{x}^{I_1}, \dots, \mathbf{x}^{I_n})$, 
    where $I_i$ are distinct degree sequences of equal 
    length, the \textit{maximal polynomial} is 
    $(m - 1)(\mathbf{x}^{I_1} + \cdots + \mathbf{x}^{I_n})$, 
    where $m$ is a non-inclusive upper bound on the coefficients.
\end{defn}

This is saying we should consider the ``worst-case'' polynomial 
for our bound-finding computations.

For small problems, we can simply plug the maximum 
polynomial $g$ into FLINT, compute $g ^ p$, and 
iterate through the resulting terms to obtain an upper bound.
We know this computation will be correct because FLINT uses GMP,
and it also must be the optimal bound on the coefficients. 
However, for larger problems, like bound finding for 
$\Delta_1$ (Algorithm \ref{alg:calc:delta1}) of
quartic K3 surfaces over $\mathbb{F}_{11}$ or $\mathbb{F}_{13}$, 
FLINT does not finish the computation in a reasonable amount of time.
If one only cares about a single shape of the problem 
(i.e. the same degree, \(k\), and characteristic),
this might be acceptable.
However, for a single computation, it completely removes 
the advantage of using the GPU, since we must perform an 
expensive CPU mutliplication to set up the algorithm.

Alternatively, we can obtain a relatively tight upper 
bound for the case of raising a homogeneous multivariate 
polynomial over $\mathbb{F}_p[x_1, \dots , x_k]$ to a power
by bounding the number of terms in the power. 

\begin{thm}
    Let $f$ be a $h$-homogeneous polynomial of $\mathbb{Z}[x_1, \dots, x_n]$, where $m$ is a non-inclusive upper bound on the coefficients. Then, the coefficients of $f ^ k$ are bounded above by $\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^ k$
\end{thm}

\begin{proof}
    We induct on $k$. The maximum coefficient 
    of $f^0$ is $1$, which is bounded above by 
    
    \noindent$\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^ 0 = 1$.
    Let $(d_1, \dots , d_n)$ denote the exponent 
    tuple of a term of $f^k$, let $(d'_1, \dots , d'_n)$ 
    denote the exponent tuple of a term of 
    $f^{k + 1}$, and let $(a_1, \dots , a_n)$ 
    denote the exponent tuple of a term of $f$.

    Consider an arbitrary term of $f^{k + 1}$. 
    To find all terms in the unreduced 
    expansion of $f^k \cdot g$ that contribute 
    to that term, we look at 
    terms of $f^k$ with degree sequences of 
    the form $(d'_1 - a_1, \dots , d'_n - a_n)$. 
    The number of these terms of $f^k$ is bounded 
    above by $|wics(h, n)|$, or $\binom{h + n - 1}{n - 1}$ 
    by Lemma \ref{lem:wics:size}. Using our 
    inductive assumption, the maximum coefficient 
    of $f^k$ is bounded above by 
    $\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^ k$, 
    so multiplying each of these by coefficients 
    of $f$, which have a maximum value of 
    $m - 1$, and adding up $\binom{h + n - 1}{n - 1}$ 
    copies of these gives 
    $\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^{k + 1}$.
\end{proof}

To apply this formula to computing $\Delta_1(f^{p - 1})$ for K3 
quartic surfaces over $\mathbb{F}_p$, we plug in $h = 4p$, $m = p$, 
and $k = p$ to obtain $M = \left((p - 1) \cdot \binom{4p + 3}{3}\right)^ k$
for an upper bound. To obtain the optimal upper bound, we can plug in the
maximal polynomial into a multimodular NTT with primes multiplying to over $M$,
and retrieve the maximum coefficient of that result.

\subsection{Other algorithms} \label{sec:poly:other}

Many other algorithms 
for polynomial powering
are described in \cite{monagan-2012-sparse-powering}. 
They compare performance in sparse and dense cases. 
Here, we breifly comment on how these algorithms perform on the GPU.
We refer the reader to \cite{monagan-2012-sparse-powering}
for a more precise description of all these algorithms.
In the following, assume we have some polynomial \(f\) which we wish 
to raise to the \(k\)-th power.

\textbf{RMUL} is analogous to the classic FOIL algorithm which is taught in schools. 
This is bottlenecked (at least on the GPU) by the ``collect like terms'' step, 
which requires a parallel sort.
While there are GPU-optimized sorting algorithms, such as the parallel merge sort
provided by CUDA.jl, it is a relatively expensive operation on the GPU.
There is an algorithm proposed in \cite{gupta-2023-gpu-sort}
which claims to beat the state-of-the-art, but we were unable to reproduce the result.

\textbf{RSQR} uses the binary expansion of \(k\) to find the power in less
total multiplications than \textbf{RMUL}.
\textbf{BINA} and \textbf{BINB} use binomial expansion to more efficiently 
expand \(f^{k}\), using \textbf{RMUL} to merge at the end.
They both perform better in the case that the problem is sparse.
On the GPU, all of these are bottlenecked by sorting, just like
\textbf{RMUL}.

\textbf{MNE} uses multinomial coefficients to expand \(f^{k}\) 
and then combines like terms with a sort. 
Unlike the previous four algorithms, it is not bottlenecked by the sort;
instead, it is bottlenecked by the memory required to store the table of
multinomial coefficients. Do note that \textbf{MNE} is 
competitive for small problems.

\textbf{SUMS} and \textbf{FPS} have dense and sparse versions which are described in 
\cite{monagan-2012-sparse-powering}.
In particular, \textbf{FPS} is implemented in FLINT, and is called by our
code when we need powering on the CPU.
They can be parallelized, as discussed in \cite{monagan-2012-sparse-powering},
but this requires the use of locks and a heap data structure,
which are more challenging to implement on the GPU.
It would be interesting to have a GPU-accelerated version of
\textbf{FPS} and compare its performance with 
multimodular FFT.
The authors expect one could get a big improvement in the sparse case.

\subsection{Evaluation}

To demonstrate the power of using the GPU for mathematical computations, 
we compare our implementation
with two existing computational mathematics libraries, FLINT
and MAGMA.
We take a homogeneous polynomial of degree 16 in the integers,
whose coefficients are randomly chosen in the set 
\(\{0, \ldots, 4\}\), and raise it to successive powers \(n\),
starting at \(n=5\).
Raising such a polynomial to the 5th power is a similar 
computation to the bottleneck in the calculation of the
quasi-\(F\)-split height of a quartic surface over \(\mathbb{F}_{5}\).

\begin{figure}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
    \hline
    \(n\) & MAGMA & FLINT & GPUPolynomials.jl \\
    \hline
    5 & 2.10 &    0.80 & 0.001 \\
    \hline                      
    6 & 5.82 &    1.64 & 0.003 \\
    \hline                      
    7 & 12.03 &   2.93 & 0.005 \\
    \hline                      
    8 & 21.64 &   5.99 & 0.010 \\
    \hline                      
    9 & 38.78 &   12.5 & 0.011 \\
    \hline                      
    10 & 63.22 &  20.7 & 0.012 \\
    \hline                      
    11 & 95.48 &  29.8 & 0.038 \\
    \hline                      
    12 & 137.61 & 40.3 & 0.043 \\
    \hline                      
    13 & 194.73 & 52.2 & 0.074 \\
    \hline                          
    14 & 267.57 & 70.0 & 0.079 \\
    \hline                          
    15 & 359.22 & 90.1 & 0.087 \\
    \hline
\end{tabular}
\caption{Polynomial powering times (in seconds) for various powers}
\end{center}
\end{figure}

Note that the above numbers are really the GPU beating the CPU.
The NTT is a dense algorithm, while FLINT uses a sparse algorithm.
We do not know which algorithm MAGMA uses for polynomial powering
in the integers.
In theory, such a sparse algorithm should
be much more efficient.
However, the throughput of the GPU is so much better that the
GPU crushes the CPU, even with a worse algorithm.
The authors hope that these numbers can inspire others who
rely on fundamental algorithms such as those in FLINT to 
consider re-implementing them on the GPU.

Additionally note that this does not mean the GPU will give 
improvements for every polynomial powering 
problem. Generally, the NTT is powerful when the resulting 
coefficients are small, and the polynomial is reasonably dense. 
In more sparse problems, as discussed before in section \ref{sec:poly:other}, 
the NTT is less effective, and other algorithms benefit less 
from parallelization, making them more 
suited for the CPU instead.
