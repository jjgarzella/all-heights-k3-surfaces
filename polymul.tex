%In this section, we explore algorithms for raising multivariate homogeneous polynomials to powers. Specifically, we consider the benefits mass parallelization using the GPU gives.

%\subsection{Kronecker Substitution and Homogeneity}

%In multivariate polynomial arithmetic, it is commonplace to map a multivariate polynomial to a univariate 
%polynomial. This is because a comparison between two exponent tuples $(d_1, \dots , d_n)$ of monomials 
%$x_1^{d_1} \dots x_n^{d_n}$ is $n$ times slower than comparing two integers. This also allows us to use the vast majority of polynomial powering algorithms, which take in univariate polynomials. One method of mapping a multivariate polynomial to a sparse univariate polynomial is the Kronecker Substitution:


In this section, we give an overview of how Algorithm \ref{alg:calc:delta1} is computed. Specifically, we focus on Step 3, \(D \gets \tilde{f}^{p}\), which turns out to be the main bottleneck. We first explain our implementation of polynomial powering using the number theoretic transform (NTT), then explore other known polynomial powering algorithms and their ability to be sped up through mass parallelization.

\subsection{Multi-modular NTT}

\subsubsection{Kronecker Substitution}
We reduce the problem of multivariate polynomial powering to univariate
by the Kronecker substitution.
Let $M$ be a non-inclusive upper bound on the degree of any variable of the polynomial
$f \in R[x_1, \dots, x_n]$. 
Then the Kronecker Substitution 
\(g(z) = f(z, z^M, z^{M^2}, \dots, z^{M^{n-1}})\)
produces a univariate polynomial $g \in R[z]$,
see \cite{arnold-2014-kronecker}.
Algebraically, this is the same as applying the 
homomorphism 
\(R[x_{1}, \ldots, x_{n}] \xrightarrow{} R[z]\)
which takes \(x_{i}\) to
\(z^{M^{i-1}}\).
This map cannot be injective in general, but it
is injective on the subset of elements of
\(f \in R[x_{1}, \ldots, x_{n}]\) 
whose terms have all degrees strictly less than \(M\).
Given two polynomials \(f_{1}, f_{2}\) that
we wish to multiply, we may choose \(M\)
big enough
so that the product lies in the subset,
so we may recover the product in 
\(R[x_{1}, \ldots, x_{n}]\) by performing
the product in the ring \(R[z]\).

\begin{rmk}
    In FLINT, exponent tuples are bitpacked into unsigned integers. The process of bitpacking an array is a Kronecker Substitution with $M$ being a power of 2.
\end{rmk}

In the case where our polynomial is homogeneous, 
we can simply ignore one variable in all of our operations between monomials of the same degree. 
As above, algebraically this is applying the evaluation
homomorphism \(x_{n} \mapsto 1\).
This map is of course not injective, but it is injective on homogeneous elements.
This effectively decreases the number of variables of our polynomial by 1, 
which greatly lowers the degree of the univariate result of the
Kronecker Substitution.
This improvement is essential for dense algorithms like the FFT.

%\subsection{NTT}

%The FFT is a very well-known algorithm, with one of its many applications being raising dense univariate polynomials to powers.
%Because we are working with integers, we use the Number Theoretic Transform (NTT), its finite field counterpart. We also refer to the NTT with ``multi-modular FFT''.
%
%To begin, we first define the length of an NTT:
%
%\begin{defn}
%    Given a NTT from $\mathbb{F}_p^L \rightarrow \mathbb{F}_p^L$, the \textbf{length} of the NTT is $L$.
%\end{defn}
%
%A quick overview of the NTT algorithm for polynomial powering is given:
%
%\begin{algorithm}[H]
%    \caption{NTT Algorithm for polynomial powering}
%    \label{nttpowalg}
%    \begin{algorithmic}[1]
%    \State \textbf{Inputs}: $f$: a dense representation of a polynomial, $p$: a NTT prime, $pow$: an integer
%    \State \textbf{Output}: dense representation of $f ^ p$
%	\State $finalDegree \gets degree(f) * pow$
%    \State $L \gets d + 1$
%    \State $f \gets padToL(f)$ \Comment{Append $0$'s to the end of $f$ until $f$ has length $L$}
%    \State $f' \gets NTT(f, p)$
%    \State $f \gets f$ .\textasciicircum $pow$ \Comment{Raise each index of $f$ to $pow$}
%    \State $f \gets INTT(f, p)$
%    \State \Return $f$
%    \end{algorithmic}
%\end{algorithm}

%To raise univariate polynomials to powers, we implement a GPU-accelerated
%multimodular NTT algorithm.
\subsubsection{Polynomial Powering using NTTs}
The \(k\)-th power of a univariate polynomial \(f\)
may be computed mod \(p\) by taking the number theoretic transform 
of a tuple containing the coefficients of \(f\),
raising the components to the \(k\),
and then computing the inverse number theoretic transform.
The multimodular algorithm 
computes \(f^{k}\) in the integers by
choosing many primes \(p_{i}\), 
and combining the results using the Chinese remainder theorem.
In practice, one uses the fast Fourier transform (FFT),
Thus, our algorithm is known as the 
multi-modular NTT approach to raising polynomials to powers.

In many cases, using the Kronecker Substitution to map a multivariate polynomial to a univariate polynomial results in 
a sparse univariate polynomial. As a result, dense algorithms like the NTT become inefficient in both time and memory complexity when compared to sparse algorithms.
However, the massive throughput of GPUs allows the NTT to be competitive for many cases, 
quartic K3 surfaces being one of them.

\subsubsection{NTT Implementation}

We use the Merge-NTT with Barrett modular reduction from \cite{ozcan-2023-fft}, ported over to Julia, for our NTT implementation. 
Because of this, our NTT length $L$ is always a power of $2$. 

For each NTT we perform, we search for primes $p$ that satisfy 
$p \equiv 1 \mod 2L$, compute primitive $L$th roots of unity 
for each $p$, and $L^{-1} \mod p$. 
These are cached, so that they can be used again for problems with the same shape 
(i.e. the degree of \(f\), the exponent \(k\), and the characteristic are the same).

% Additionally, we used fixed-width integers, as opposed to arbitrary-precision integers 
% such as in the GNU Multiple Precision library (GMP) \cite{gnu-2024-gmp}. The decision
% to use fixed-width integers is primarily practical. 
% Arbitrary size fixed-width integers
% on the GPU are provided by the BitIntegers.jl package 
% \cite{fourquet-2024-bitintegers-jl}.
% On the other hand, Julia's CUDA functionality currently does not provide
% aribtrary-precision integers:
% libraries like GMP only support the CPU, while 
% Nvidia-made alternatives have not yet been ported to Julia.
% On the CPU, fixed-width integers of 256, 512, or 1024 bits
% can outperform arbitrary-precision libraries like GMP 
% since fixed-width integers don't require allocations.
% It would be interesting to compare the performance of both
% approaches on the GPU.

Our algorithm also has fallbacks when the memory required becomes too large for the 
GPU's on-device memory.
When the problem becomes too big to fit the twiddle factors for 
all of the NTTs
in device memory at once, we move the twiddle factors to GPU memory and 
back to RAM for each NTT. Because we suddenly are forced to move a lot 
of memory around for each NTT, we see a large drop-off in 
performance when this 
NTT size threshold is reached. 


\subsubsection{Prime Selection}
In order for the NTT to simulate polynomial powering over $\mathbb{Z}$, 
we need to obtain an upper bound $M$ on the resulting coefficients, then 
choose primes $p_1 \dots p_k$ such that $p_1 \cdot p_2 \cdot ... \cdot p_k > M$

Experimentally, modular multiplication with 32-bit integers 
is slightly over double the speed of modular 
multiplication with 64-bit integers. 
However, the kernels and parameters from Merge-NTT are more 
optimized for 64-bit integers, making them just over half the
speed of naively using the same implementation with 32-bit 
integers. Thus, for prime selection, 
we choose primes satisfying $p \equiv 1 \mod 2L$ that fit 
within 62 bits, because of the precision of Barrett Reduction.

\subsubsection{Bound Finding}
%This poses the question of finding the maximum coefficient of our result $M$. 
The multimodular NTT requires an upper bound $M$ in order to select primes 
to compute NTTs in. In this section, we present a quick way to compute an 
upper bound on the resulting coefficients of raising a polynomial to a power.

\begin{defn}
    Given a basis $(\mathbf{x}^{I_1}, \dots, \mathbf{x}^{I_n})$, where $I_i$ are distinct degree sequences of equal length, the maximum polynomial is $(m - 1)(\mathbf{x}^{I_1} + \cdots + \mathbf{x}^{I_n})$, where $m$ is a non-inclusive upper bound on the coefficients.
\end{defn}

This is effectively just saying to consider the worst-case polynomial 
for our bound-finding computations.

For small problems, we can simply plug the maximum polynomial $g$ into FLINT, compute $g ^ p$, and 
iterate through the resulting terms to obtain an upper bound.
We know this computation will be correct because FLINT uses GMP,
and it also must be the optimal bound on the coefficients. 
However, for larger problems, like bound finding for quartic K3 surfaces over $\mathbb{F}_{11}$ or $\mathbb{F}_{13}$, FLINT does not finish the computation in a reasonable amount of time.
If one only cares about a single shape of the problem (i.e. the same degree, \(k\), and charachteristic),
this might be acceptable.
However, for a single computation, it completely removes the advantage of using the GPU,
since we must perform an expensive CPU mutliplication to set up the algorithm.

Alternatively, we can obtain a relatively tight upper bound for the case of raising a homogeneous multivariate polynomial over $\mathbb{F}_p[x_1, \dots , x_k]$ to a power
by bounding the number of terms in the power. 

\begin{thm}
    Let $f$ be a $h$-homogeneous polynomial of $\mathbb{Z}[x_1, \dots, x_n]$, where $m$ is a non-inclusive upper bound on the coefficients. Then, the coefficients of $f ^ k$ are bounded above by $\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^ k$
\end{thm}

\begin{proof}
    We induct on $k$. The maximum coefficient of $f^0$ is $1$, which is bounded above by 
    
    \noindent$\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^ 0 = 1$.
    Let $(d_1, \dots , d_n)$ denote the exponent tuple of a term of $f^k$, let $(d'_1, \dots , d'_n)$ denote the exponent tuple of a term of $f^{k + 1}$, and let $(a_1, \dots , a_n)$ denote the exponent tuple of a term of $f$.

    Consider an arbitrary term of $f^{k + 1}$. To find all terms in the unreduced expansion of $f^k \cdot g$ that contribute to that term, we look at terms of $f^k$ with degree sequences of the form $(d'_1 - a_1, \dots , d'_n - a_n)$. The number of these terms of $f^k$ is bounded above by $|wics(h, n)|$, or $\binom{h + n - 1}{n - 1}$ by Lemma \ref{lem:wics:size}. Using our inductive assumption, the maximum coefficient of $f^k$ is bounded above by $\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^ k$, so multiplying each of these by coefficients of $f$, which have a maximum value of $m - 1$, and adding up $\binom{h + n - 1}{n - 1}$ copies of these gives $\left((m - 1) \cdot \binom{h + n - 1}{n - 1}\right)^{k + 1}$.
\end{proof}

To apply this formula to computing $\Delta_1(f^{p - 1})$ for K3 
quartic surfaces over $\mathbb{F}_p$, we plug in $h = 4p$, $m = p$, 
and $k = p$ to obtain $M = \left((p - 1) \cdot \binom{4p + 3}{3}\right)^ k$
for an upper bound. To obtain the optimal upper bound, we can plug in the
maximal polynomial into a multimodular NTT with primes multiplying to over $M$,
and retrieve the maximum coefficient of that result.

\subsection{Other algorithms} \label{sec:poly:other}

Many other algorithms 
for polynomial powering
are described in \cite{monagan-2012-sparse-powering}. 
They compare performance in sparse and dense cases. 
Here, we breifly comment on how these algorithms perform on the GPU.
We refer the reader to \cite{monagan-2012-sparse-powering}
for a more precise description of all these algorithms.
In the following, assume we have some polynomial \(f\) which we wish 
to raise to the \(k\)-th power.

\textbf{RMUL} is analogous to the classic FOIL algorithm which is taught in schools. 
This is bottlenecked (at least on the GPU) by the ``collect like terms'' step, 
which requires a parallel sort.
While there are GPU-optimized sorting algorithms, such as the parallel merge sort
provided by CUDA.jl, it is a relatively expensive operation on the GPU.
There is an algorithm proposed in \cite{gupta-2023-gpu-sort}
which claims to beat the state-of-the-art, but we were unable to reproduce the result.

\textbf{RSQR} uses the binary expansion of \(k\) to find the power in less
total multiplications than \textbf{RMUL}.
\textbf{BINA} and \textbf{BINB} use binomial expansion to more efficiently 
expand \(f^{k}\), using \textbf{RMUL} to merge at the end.
They both perform better in the case that the problem is sparse.
On the GPU, all of these are bottlenecked by sorting, just like
\textbf{RMUL}.

\textbf{MNE} uses multinomial coefficients to expand \(f^{k}\) 
and then combines like terms with a sort. 
Unlike the previous four algorithms, it is not bottlenecked by the sort;
instead, it is bottlenecked by the memory required to store the table of
multinomial coefficients. However, \textbf{MNE} is 
competitive for small problems

\textbf{SUMS} and \textbf{FPS} have dense and sparse versions which are described in 
\cite{monagan-2012-sparse-powering}.
In particular, \textbf{FPS} is implemented in FLINT, and is called by our
code when we need powering on the CPU.
They can be parallelized, as discussed in \cite{monagan-2012-sparse-powering},
but this requires the use of locks and a heap data structure,
which are more challenging to implement on the GPU.
It would be interesting to have a GPU-accelerated version of
\textbf{FPS} and compare it's performance with 
multimodular FFT.
The authors expect one could get a big improvement in the sparse case.

\subsection{Evaluation}

To demonstrate the power of using the GPU for mathematical computations, 
we compare our implementation
with two existing computational mathematics libraries, FLINT
and MAGMA.
We take a homogeneous polynomial of degree 16 in the integers,
whose coefficients are randomly chosen in the set 
\(\{0, \ldots, 4\}\), and raise it to successive powers \(n\),
starting at \(n=5\).
Raising such a polynomial to the 5th power is a similar 
computation to the bottleneck in the calculation of the
quasi-\(F\)-split height of a quartic surface over \(\mathbb{F}_{5}\).

\begin{figure}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
    \hline
    \(n\) & MAGMA & FLINT & GPUPolynomials.jl \\
    \hline
    5 & 2.10 &    0.87 &  0.03 \\
    \hline
    6 & 5.82 &    1.76 &  0.04 \\
    \hline
    7 & 12.03 &   3.82 &  0.12 \\
    \hline
    8 & 21.64 &   6.50 &  0.20 \\
    \hline
    9 & 38.78 &   12.32 & 0.40 \\
    \hline
    10 & 63.22 &  18.99 & 0.47 \\
    \hline
    11 & 95.48 &  26.58 & 0.89 \\
    \hline
    12 & 137.61 & 35.70 & 1.14 \\
    \hline
    13 & 194.73 & 46.50 & 2.07 \\
    \hline
    14 & 267.57 & 60.07 & 2.55 \\
    \hline
    15 & 359.22 & 76.46 & 2.77 \\
    \hline
\end{tabular}
\caption{Polynomial powering times (in seconds) for various powers}
\end{center}
\end{figure}

Note that the above numbers are really the GPU beating the CPU.
The FFT is a dense algorithm, while FLINT uses a sparse algorithm.
We do not know which algorithm MAGMA uses for polynomial powering
in the integers.
In theory, such a sparse algorithm should
be much more efficient.
However, the parallelism of the GPU is so much better that the
GPU crushes the CPU, even with a worse algorithm.
The authors hope that these numbers can inspire others who
rely on fundamental algorithms such as those in FLINT to 
consider re-implementing them on the GPU.

Additionally note that this does not mean the GPU will give 
improvements for every polynomial powering 
problem. Generally, the NTT is powerful when the resulting 
coefficients are small, and the polynomial is reasonably dense. 
In more sparse problems, as discussed before in section \ref{sec:poly:other}, 
the algorithms benefit less from parallelization, making them more 
suited for the CPU instead.